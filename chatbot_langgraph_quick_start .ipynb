{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW8/gtG0jHaL8oFE4dJ3P+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/LangGraph/blob/main/chatbot_langgraph_quick_start%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Tutorials](https://langchain-ai.github.io/langgraph/tutorials/)\n",
        "\n",
        "[Quick Start](https://langchain-ai.github.io/langgraph/tutorials/introduction/)"
      ],
      "metadata": {
        "id": "PbnBZo4Dwbgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chatbot**\n",
        "\n",
        "00_quickstart_part1_to_part7.ipynb\n",
        "\n",
        "[Chatbot - LangGraph Quick Start - rm Github](https://github.com/raheelam98/LangGraph/blob/main/langchain_ecosystem/langgraph/chatbot/docs/00_quickstart_part1_to_part7.ipynb)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z61ytMwcuh3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jHxnnrhS0aDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the required packages:\n",
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langsmith langchain_google_genai"
      ],
      "metadata": {
        "id": "2RABOES20T6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys\n",
        "# Get the GEMINI API key from user data\n",
        "import os\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "0e7C34pA0m3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')"
      ],
      "metadata": {
        "id": "GLuocEE-0olo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check each set is running\n",
        "import os\n",
        "# from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"quickstart\""
      ],
      "metadata": {
        "id": "sMBEY6fe0x47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ChatGoogleGenerativeAI with the Gemini model\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    max_retries=2,\n",
        "    api_key=gemini_api_key    # Provide the Google API key for authentication\n",
        ")"
      ],
      "metadata": {
        "id": "U--paUBh1DVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"hello\")"
      ],
      "metadata": {
        "id": "5jY6pc-S2D3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LLM with a query\n",
        "result = llm.invoke(\"Rain is expected in Karachi starting today?\")\n",
        "result"
      ],
      "metadata": {
        "id": "ib6hfxiH1HA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ],
      "metadata": {
        "id": "MK0wrf0y1PH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "LANGSMITH_API_KEY= userdata.get('LANGSMITH_API_KEY')"
      ],
      "metadata": {
        "id": "3zO-4AMN1Ug-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chatbot with Tools :** Integrate a web search tool into the bot.\n",
        "\n",
        "First, install the requirements to use the [Tavily Search Engine](https://python.langchain.com/docs/integrations/tools/tavily_search/), and set your [TAVILY_API_KEY](https://tavily.com/)."
      ],
      "metadata": {
        "id": "e9j-dgfx1gRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot with Tools : Integrate a web search tool into the bot.\n",
        "%%capture --no-stderr\n",
        "%pip install -U tavily-python langchain_community"
      ],
      "metadata": {
        "id": "Ce2TYqWQ1jv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API keys set up\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "id": "SSgTPAbu1o-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "#tool.invoke(\"What's a 'node' in LangGraph?\")\n",
        "tool.invoke(\"Rain expected in Karachi this week\")"
      ],
      "metadata": {
        "id": "OvsUL5Yy1tjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Adding Memory to the Chatbot\n",
        "\n",
        "**Provide a `checkpointer` when compiling the graph and a `thread_id` when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same `thread_id`, the graph loads its saved state, allowing the chatbot to pick up where it left off.**"
      ],
      "metadata": {
        "id": "A9otfdjfxr-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "**Integrating memory checkpointing**\n",
        "\n",
        "**`graph = graph_builder.compile(checkpointer=MemorySaver()) `** compiles the graph with memory checkpointing, allowing it to save and restore the state of the graph's nodes during its execution."
      ],
      "metadata": {
        "id": "ceiQx3Tg5S2M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdEoURfJuUVm"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can interact with your bot! First, pick a thread to use as the key for this conversation."
      ],
      "metadata": {
        "id": "tfBelLT9yJ3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}"
      ],
      "metadata": {
        "id": "rxlhmnk-yFRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, call your chat bot."
      ],
      "metadata": {
        "id": "AePI9oSkyOxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start conversation\n",
        "user_input = \"Hi there! My name is Bushra.\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "I6FDCGdUx--q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The config was provided as the second positional argument when calling our graph. It importantly is not nested within the graph inputs ({'messages': []}).\n",
        "\n",
        "Let's ask a followup: see if it remembers your name."
      ],
      "metadata": {
        "id": "I6Jizu0myWJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Remember my name?\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "a0noTlVbyCK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Human-in-the-loop\n",
        "\n",
        "LangGraph supports `human-in-the-loop` workflows in a number of ways. In this section, we will use LangGraph's `interrupt_before` functionality to always break the tool node."
      ],
      "metadata": {
        "id": "siKu-EIyyiFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  compile the graph, specifying to interrupt_before the tools node.\n",
        "graph = graph_builder.compile(\n",
        "    checkpointer=memory,\n",
        "    # This is new!\n",
        "    interrupt_before=[\"tools\"],\n",
        "    # Note: can also interrupt __after__ tools, if desired.\n",
        "    # interrupt_after=[\"tools\"]\n",
        ")"
      ],
      "metadata": {
        "id": "SvMnIVJPz1Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I'm learning LangGraph. Could you do some research on it for me?\"\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "7UoG_4JP0AcE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}