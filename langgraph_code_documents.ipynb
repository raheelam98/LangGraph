{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGT8AaGZOyB5vy+nd8SYCb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/LangGraph/blob/main/langgraph_code_documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 1 :**  5_agent_memory.ipynb\n",
        "\n",
        "[Class-04: Mastering LangGraph In a New Way: Memory, RAG, Fine Tuning, GraphRAG, AI Agents - Nov 15, 2024  ](https://www.youtube.com/watch?v=Zf2C8A6RmJE)"
      ],
      "metadata": {
        "id": "rg-BpnmZzuxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from langgraph and langchain_core\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# # System message (use to increase performance)\n",
        "# Define a system message to establish the assistant's role\n",
        "# This message sets the context for the assistant's behavior\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# # Node (create assistant node)\n",
        "# Define a function named 'assistant' that takes a MessagesState as input\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "    # Invoke the LLM with tools using the system message and the current state of messages\n",
        "    # The response is returned in a dictionary format, appended to the 'messages' key\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"]) ] }\n",
        "\n",
        "    # Main :- llm_with_tools.invoke([sys_msg] + state[\"messages\"])\n",
        "    # ensures the assistant uses both the predefined context and the conversation history to respond appropriately\n",
        "\n",
        "    # step 1 : System Message Inclusion: :- llm_with_tools.invoke([sys_msg]\n",
        "    # calls the language model with just the system message, which establishes the context for the assistant.\n",
        "\n",
        "    # step 2 : Combined Messages Processing :-  llm_with_tools.invoke(state[\"messages\"])\n",
        "    # processes the current conversation history (state messages)\n",
        "    # by including them along with the system message to generate a coherent\n",
        "    # response considering both the context and the history.\n"
      ],
      "metadata": {
        "id": "JpBr_ize0XPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "```bash\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# # System message (create system message for better performance)\n",
        "sys_msg = SystemMessage(content=\"You message.\")\n",
        "```\n",
        "\n",
        "Node (create assistant node)\n",
        "\n",
        "**Functionality :** Takes input, combines it with the system message, sends them to the LLM, and stores the LLM's response.\n",
        "\n",
        "When the function gets the prompt, it sends it to the LLM along with both the system message and the prompt\n",
        "\n",
        "\n",
        "```bash\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
        "```\n",
        "\n",
        "**Detail**\n",
        "\n",
        "Define a function named 'assistant' that takes a MessagesState as input\n",
        "\n",
        "**`def assistant(state: MessagesState) -> MessagesState:`**\n",
        "\n",
        "This code calls the language model with both the system message and the current conversation history to generate a relevant response.\n",
        "\n",
        "**`[llm_with_tools.invoke([sys_msg] + state[\"messages\"]) ]`**\n",
        "\n",
        "\n",
        "**Combined Messages Processing :**\n",
        "**`[llm_with_tools.invoke([sys_msg] + state[\"messages\"])]`**\n",
        "\n",
        "This step makes sure the assistant takes into account both the initial instructions and the entire conversation so far to respond appropriately.\n",
        "\n",
        "* **Including the System Message : `(sys_msg)`** Adding the initial instruction **`(sys_msg)`** to the list of messages. This sets the context for the assistant.\n",
        "\n",
        "* **Processing Conversation History : `state[\"messages\"]`** Combining the system message with the existing conversation history (all previous messages in **`state[\"messages\"]`**). This ensures the assistant understands the context and history to generate a relevant and coherent response.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "kuCZ8ot_zx1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm note**\n",
        "\n",
        "**Don't retain memory from the initial chat!** because it don't has memory only hase state\n",
        "\n",
        "When the graph starts and runs, after finishing, the graph state is lost because it gets overwritten.\n",
        "\n",
        "**Transient State :** Starts a new chat session.\n",
        "\n",
        "**Steady State :** Remembers details from earlier conversations\n"
      ],
      "metadata": {
        "id": "OkoJwxtCGD8V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8Vdw_qSWygRo"
      },
      "outputs": [],
      "source": [
        "# Steady State : Remembers details from earlier conversations\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory: MemorySaver = MemorySaver()\n",
        "react_graph_memory: CompiledStateGraph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "To retain conversations, LangGraph uses **`persistent checkpointing`**.\n",
        "\n",
        "```bash\n",
        "react_graph_memory: CompiledStateGraph = builder.compile(checkpointer=memory)\n",
        "```\n",
        "\n",
        "LangGraph automatically saves the state after each step. When the graph is invoked again using the same **`thread_id`**, it loads its saved state, allowing the chatbot to continue from where it left off"
      ],
      "metadata": {
        "id": "CLLrcsKHHmmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 1 :**  5_agent_memory.ipynb\n",
        "\n",
        "When we use memory, we need to specify a `thread_id`.\n",
        "\n",
        "This **`thread_id`** will store our memory location of graph states.\n",
        "\n",
        "Here is a cartoon:\n",
        "\n",
        "* The checkpointer write the state at every step of the graph\n",
        "* These checkpoints are saved in a thread\n",
        "* We can access that thread in the future using the `thread_id`\n",
        "\n",
        "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)\n"
      ],
      "metadata": {
        "id": "IyHXPX71Nxd6"
      }
    }
  ]
}