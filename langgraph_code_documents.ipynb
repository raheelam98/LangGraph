{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTNjS+6lQ6awNed/RBsDev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/LangGraph/blob/main/langgraph_code_documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm note**\n",
        "\n",
        "state:- structure of work flow\n",
        "\n",
        "memory :- presisten memory (store things)\n",
        "\n",
        "API :- talk one meachine to another meachine"
      ],
      "metadata": {
        "id": "Qr-iH133ueA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check it later\n",
        "# module 1 : 2.2_chains_reducers.ipynb\n",
        "\n",
        "llm_with_tools: ChatGoogleGenerativeAI = llm.bind_tools([multiply])"
      ],
      "metadata": {
        "id": "xzKpNA4s20ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ## langchain-academy\n",
        "\n",
        "[LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph)\n",
        "  \n",
        "[langchain-academy - Github](https://github.com/langchain-ai/langchain-academy/tree/main)"
      ],
      "metadata": {
        "id": "b_KU43rO4lh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required packages:\n",
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langsmith # check\n",
        "%pip install --quiet -U langchain_google_genai langchain_core langgraph"
      ],
      "metadata": {
        "id": "VTp5u3K70cwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description of Libraries**\n",
        "* `langchain-google-genai:` LangChain doesn't have its own chat model, so we use Google's Gemini.\n",
        "* `langchain-core`: Utilizes LangChain core libraries, e.g., human-message, ai-message.\n",
        "* `langchain_community:` Tavily-python is part of the LangChain community package.\n",
        "* `tavily-python: `Used for web search.\n",
        "Note: Use both langchain_community and tavily-python together.\n"
      ],
      "metadata": {
        "id": "6kcY9QzS5adc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"project_name\""
      ],
      "metadata": {
        "id": "kJqgQQ3d0lN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys\n",
        "# Get the GEMINI API key from user data\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "nWMujpLX1TBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ],
      "metadata": {
        "id": "krx7ywtn1eGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "LANGCHAIN_API_KEY= userdata.get('LANGCHAIN_API_KEY')"
      ],
      "metadata": {
        "id": "dROJZ5v-1UOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ChatGoogleGenerativeAI with the Gemini model\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    max_retries=2,\n",
        "    api_key=gemini_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "bsAhvpzJ1n-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LLM with a query\n",
        "result = llm.invoke(\"Rain is expected in Karachi starting today?\")\n",
        "result"
      ],
      "metadata": {
        "id": "h071NcYm1pYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detail of each module"
      ],
      "metadata": {
        "id": "wvfJuED92OGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Moduel 0**\n",
        "\n",
        "[Class-01: Mastering LangGraph In a New Way: Core Concepts & Implementation Node, Edges, States - Nov 8, 2024](https://www.youtube.com/watch?v=jIX9P12IkQM)"
      ],
      "metadata": {
        "id": "xhMz2sO53QMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Description of LLM methods**\n",
        "* `stream`: Stream back chunks of the response (useful for streaming results).\n",
        "* `invoke`: Call the chain on an input (provides a complete result from start to end).\n",
        "\n",
        "#### **Search Tools - TAVILY**\n",
        "\n",
        "* TAVILY is a search engine created by the LangChain open community. It provides the latest accurate data from the web.\n",
        "* LLMs do not have access to real-time information, e.g., weather forecasts.\n",
        "* LLMs do not have the latest information. That's why we use TAVILY to access real-time information, e.g., weather forecasts.\n"
      ],
      "metadata": {
        "id": "YNxG7b3K341m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the TAVILY API key from user data and set the environment variable\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "Tk6WkB-W3NAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 1 :** 2.2_chains_reducers.ipynb\n",
        "\n",
        "[Class-02: Mastering LangGraph In a New Way: LLM with Tool Calling, Chains & Reducers - Nov 9, 2024 ](https://www.youtube.com/watch?v=g1GqJx2k1Hk)\n"
      ],
      "metadata": {
        "id": "oaGxZYyY6CNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using messages as state\n",
        "\n",
        "With these foundations in place, we can now use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages) in our graph state.\n",
        "\n",
        "Let's define our state, `MessagesState`, as a `TypedDict` with a single key: `messages`.\n",
        "\n",
        "`messages` is simply a list of messages, as we defined above (e.g., `HumanMessage`, etc)."
      ],
      "metadata": {
        "id": "u0hAEpTu43tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: list[AnyMessage]"
      ],
      "metadata": {
        "id": "5R86JDLj7Rjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducers address the issue of **new state values overriding prior ones**. By using the pre-built **`add_messages` reducer**, we ensure that **messages are appended to the existing list** rather than replaced. We just need to **annotate the messages key with this reducer function as metadata**.\n",
        "\n",
        "```bash\n",
        " messages: Annotated[list[AnyMessage], add_messages]\n",
        " ```"
      ],
      "metadata": {
        "id": "rOLmVTLY7Xp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules for typing and LangGraph\n",
        "from typing import Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# Define a TypedDict class 'MessagesState' to store messages\n",
        "class MessagesState(TypedDict):\n",
        "    # Annotate the 'messages' key to use the 'add_messages' reducer function for appending messages\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n"
      ],
      "metadata": {
        "id": "yDr5tKqQ9LRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 1 :**  5_agent_memory.ipynb\n",
        "\n",
        "[Class-04: Mastering LangGraph In a New Way: Memory, RAG, Fine Tuning, GraphRAG, AI Agents - Nov 15, 2024  ](https://www.youtube.com/watch?v=Zf2C8A6RmJE)"
      ],
      "metadata": {
        "id": "rg-BpnmZzuxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from langgraph and langchain_core\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# # System message (use to increase performance)\n",
        "# Define a system message to establish the assistant's role\n",
        "# This message sets the context for the assistant's behavior\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# # Node (create assistant node)\n",
        "# Define a function named 'assistant' that takes a MessagesState as input\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "    # Invoke the LLM with tools using the system message and the current state of messages\n",
        "    # The response is returned in a dictionary format, appended to the 'messages' key\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"]) ] }\n",
        "\n",
        "    # Main :- llm_with_tools.invoke([sys_msg] + state[\"messages\"])\n",
        "    # ensures the assistant uses both the predefined context and the conversation history to respond appropriately\n",
        "\n",
        "    # step 1 : System Message Inclusion: :- llm_with_tools.invoke([sys_msg]\n",
        "    # calls the language model with just the system message, which establishes the context for the assistant.\n",
        "\n",
        "    # step 2 : Combined Messages Processing :-  llm_with_tools.invoke(state[\"messages\"])\n",
        "    # processes the current conversation history (state messages)\n",
        "    # by including them along with the system message to generate a coherent\n",
        "    # response considering both the context and the history.\n"
      ],
      "metadata": {
        "id": "JpBr_ize0XPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "```bash\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# # System message (create system message for better performance)\n",
        "sys_msg = SystemMessage(content=\"You message.\")\n",
        "```\n",
        "\n",
        "Node (create assistant node)\n",
        "\n",
        "**Functionality :** Takes input, combines it with the system message, sends them to the LLM, and stores the LLM's response.\n",
        "\n",
        "When the function gets the prompt, it sends it to the LLM along with both the system message and the prompt\n",
        "\n",
        "\n",
        "```bash\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
        "```\n",
        "\n",
        "**Detail**\n",
        "\n",
        "Define a function named 'assistant' that takes a MessagesState as input\n",
        "\n",
        "**`def assistant(state: MessagesState) -> MessagesState:`**\n",
        "\n",
        "This code calls the language model with both the system message and the current conversation history to generate a relevant response.\n",
        "\n",
        "**`[llm_with_tools.invoke([sys_msg] + state[\"messages\"]) ]`**\n",
        "\n",
        "\n",
        "**Combined Messages Processing :**\n",
        "**`[llm_with_tools.invoke([sys_msg] + state[\"messages\"])]`**\n",
        "\n",
        "This step makes sure the assistant takes into account both the initial instructions and the entire conversation so far to respond appropriately.\n",
        "\n",
        "* **Including the System Message : `(sys_msg)`** Adding the initial instruction **`(sys_msg)`** to the list of messages. This sets the context for the assistant.\n",
        "\n",
        "* **Processing Conversation History : `state[\"messages\"]`** Combining the system message with the existing conversation history (all previous messages in **`state[\"messages\"]`**). This ensures the assistant understands the context and history to generate a relevant and coherent response.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "kuCZ8ot_zx1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm note**\n",
        "\n",
        "**Don't retain memory from the initial chat!** because it don't has memory only hase state\n",
        "\n",
        "When the graph starts and runs, after finishing, the graph state is lost because it gets overwritten.\n",
        "\n",
        "**Transient State :** Starts a new chat session.\n",
        "\n",
        "**Steady State :** Remembers details from earlier conversations\n"
      ],
      "metadata": {
        "id": "OkoJwxtCGD8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "\n",
        "To retain conversations, LangGraph uses **`persistent checkpointing`**.\n",
        "\n",
        "```bash\n",
        "react_graph_memory: CompiledStateGraph = builder.compile(checkpointer=memory)\n",
        "```\n",
        "\n",
        "LangGraph automatically saves the state after each step. When the graph is invoked again using the same **`thread_id`**, it loads its saved state, allowing the chatbot to continue from where it left off\n",
        "\n",
        "**Note : Sequential tasks in LangGraph :**\n",
        "\n",
        "**LangGraph only performs parallel tasks**. To make sequential tasks, we have to design the architecture to make it sequential\n",
        "\n",
        "**State**\n",
        "\n",
        "**Note : Checkpoints are Saved in a thread:**\n",
        "\n",
        "Threads act as memory locations where the state is saved.\n",
        "\n",
        "Through the **`thread_id`**, the state is saved into the memory location.\n",
        "\n",
        "**Note : MemorySaver :**\n",
        "\n",
        "MemorySaver retains the state while connected. Once disconnected, the data is lost.\n",
        "\n",
        "\n",
        "This means the memory is not persistent and does not save the data when the connection is terminated. For persistent memory, you would need to implement a different solution. (module 2 )"
      ],
      "metadata": {
        "id": "ONUfjR5LDrkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Vdw_qSWygRo"
      },
      "outputs": [],
      "source": [
        "# Steady State : Remembers details from earlier conversations\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory: MemorySaver = MemorySaver()\n",
        "react_graph_memory: CompiledStateGraph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 1 :**  5_agent_memory.ipynb\n",
        "\n",
        "**Tutorial**\n",
        "\n",
        "When we use memory, we need to specify a `thread_id`.\n",
        "\n",
        "This **`thread_id`** will store our memory location of graph states.\n",
        "\n",
        "Here is a cartoon:\n",
        "\n",
        "* The checkpointer write the state at every step of the graph\n",
        "* These checkpoints are saved in a thread\n",
        "* We can access that thread in the future using the `thread_id`\n",
        "\n",
        "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)\n"
      ],
      "metadata": {
        "id": "IyHXPX71Nxd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}"
      ],
      "metadata": {
        "id": "3mKJfxCJZURW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify an input\n",
        "# Create a list of messages with a HumanMessage containing the content \"Add 3 and 4.\"\n",
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "\n",
        "# Run\n",
        "# Invoke the graph with the specified messages and configuration\n",
        "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
        "\n",
        "# Iterate through the resulting messages\n",
        "for m in messages['messages']:\n",
        "    # Print each message in a formatted ma\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "gqxoO52KZ3_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}