{ 
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/LangGraph/blob/main/langgraph_code_documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm note**\n",
        "\n",
        "state:- structure of work flow\n",
        "\n",
        "memory :- presisten memory (store things)\n",
        "\n",
        "API :- talk one meachine to another meachine"
      ],
      "metadata": {
        "id": "Qr-iH133ueA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(builder))"
      ],
      "metadata": {
        "id": "g0jNi-tYjgcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangGraph Academy Videos**\n",
        "\n",
        "Module 2 - State Reducer - Lesson 2\n",
        "\n",
        "**Reducer : update state**\n",
        "* Update state for both a single node and simultaneously in two nodes.\n"
      ],
      "metadata": {
        "id": "QbP6QynamsbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ## langchain-academy\n",
        "\n",
        "[LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph)\n",
        "  \n",
        "[langchain-academy - Github](https://github.com/langchain-ai/langchain-academy/tree/main)"
      ],
      "metadata": {
        "id": "b_KU43rO4lh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required packages:\n",
        "%%capture --no-stderr\n",
        "%pip install -U  langsmith # check\n",
        "%pip install --quiet -U  langchain_google_genai langchain_core langgraph"
      ],
      "metadata": {
        "id": "VTp5u3K70cwH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description of Libraries**\n",
        "* `langchain-google-genai:` LangChain doesn't have its own chat model, so we use Google's Gemini.\n",
        "* `langchain-core`: Utilizes LangChain core libraries, e.g., human-message, ai-message.\n",
        "* `langchain_community:` Tavily-python is part of the LangChain community package.\n",
        "* `tavily-python: `Used for web search.\n",
        "Note: Use both langchain_community and tavily-python together.\n"
      ],
      "metadata": {
        "id": "6kcY9QzS5adc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"project_name\""
      ],
      "metadata": {
        "id": "kJqgQQ3d0lN1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys\n",
        "# Get the GEMINI API key from user data\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "nWMujpLX1TBr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ChatGoogleGenerativeAI with the Gemini model\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    max_retries=2,\n",
        "    api_key=gemini_api_key    # Provide the Google API key for authentication\n",
        ")"
      ],
      "metadata": {
        "id": "0ODu7vN-k2om"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LLM with a query\n",
        "result = llm.invoke(\"Rain is expected in Karachi starting today?\")\n",
        "result"
      ],
      "metadata": {
        "id": "h071NcYm1pYS",
        "outputId": "a6dd5d02-f78a-42a8-9501-8fefac842c48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I do not have access to real-time information, including current weather forecasts.  To find out if rain is expected in Karachi starting today, please check a reliable weather source such as:\\n\\n* **A reputable weather website:**  Search for \"Karachi weather forecast\" on Google or your preferred search engine.  Many websites, like AccuWeather, The Weather Channel, or similar services specific to Pakistan, will provide detailed forecasts.\\n* **A weather app:** Many weather apps for smartphones provide up-to-the-minute forecasts.\\n\\nThese sources will give you the most accurate and up-to-date information.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-be837cac-2756-4d6c-b107-dcadcd4de278-0', usage_metadata={'input_tokens': 9, 'output_tokens': 127, 'total_tokens': 136, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ],
      "metadata": {
        "id": "krx7ywtn1eGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "\n",
        "LANGSMITH_API_KEY= userdata.get('LANGSMITH_API_KEY')"
      ],
      "metadata": {
        "id": "7Ejq8NB5b6I7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chatbot with Tools :** Integrate a web search tool into the bot.\n",
        "\n",
        "First, install the requirements to use the [Tavily Search Engine](https://python.langchain.com/docs/integrations/tools/tavily_search/), and set your [TAVILY_API_KEY](https://tavily.com/)."
      ],
      "metadata": {
        "id": "oFtVerpWdTAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # packages\n",
        "%%capture --no-stderr\n",
        "%pip install -U tavily-python langchain_community"
      ],
      "metadata": {
        "id": "r1QqLgQodSl2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API keys set up\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "id": "-uXI-cmydc4d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "#tool.invoke(\"What's a 'node' in LangGraph?\")\n",
        "tool.invoke(\"Rain expected in Karachi this week\")\n"
      ],
      "metadata": {
        "id": "-Uvy2Cc4huW-",
        "outputId": "cfdddea8-4648-4f96-edaa-447c36044cd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://www.weather-forecast.com/locations/Karachi/forecasts/latest',\n",
              "  'content': '12 day Karachi Weather Forecast. Live Weather Warnings, hourly weather updates. Accurate Karachi weather today, forecast for sun, rain, wind and temperature. English; ... Karachi Weather Next Week (10-12 days) Mostly dry. Warm (max 28°C on Tue afternoon, min 21°C on Mon morning). Wind will be generally light. °C. Fri. 06. Saturday. 07'},\n",
              " {'url': 'https://www.timeanddate.com/weather/pakistan/karachi/ext',\n",
              "  'content': '(Weather station: Karachi Airport, Pakistan). ... See weather overview. 2 Week Extended Forecast in Karachi, Sindh, Pakistan. Scroll right to see more Conditions Comfort Precipitation Sun; Day Temperature Weather Feels Like Wind Humidity Chance Amount UV Sunrise Sunset; Sat Dec 7: 85 / 59 °F: Scattered clouds. 81 °F: 11 mph:'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "pprint(vars(llm))  # Prints all attributes of the llm object in a structured way\n"
      ],
      "metadata": {
        "id": "DmrLHab6mIl9",
        "outputId": "62588a9f-bd1d-408c-c0fd-ca81ba51332c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'_serialized': {'id': ['langchain_google_genai',\n",
            "                        'chat_models',\n",
            "                        'ChatGoogleGenerativeAI'],\n",
            "                 'kwargs': {'default_metadata': [],\n",
            "                            'google_api_key': {'id': ['GOOGLE_API_KEY'],\n",
            "                                               'lc': 1,\n",
            "                                               'type': 'secret'},\n",
            "                            'max_retries': 2,\n",
            "                            'model': 'models/gemini-1.5-flash',\n",
            "                            'n': 1,\n",
            "                            'temperature': 0.7},\n",
            "                 'lc': 1,\n",
            "                 'name': 'ChatGoogleGenerativeAI',\n",
            "                 'type': 'constructor'},\n",
            " 'additional_headers': None,\n",
            " 'async_client_running': None,\n",
            " 'cache': None,\n",
            " 'cached_content': None,\n",
            " 'callback_manager': None,\n",
            " 'callbacks': None,\n",
            " 'client': <google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7af2095a0eb0>,\n",
            " 'client_options': None,\n",
            " 'convert_system_message_to_human': False,\n",
            " 'credentials': None,\n",
            " 'custom_get_token_ids': None,\n",
            " 'default_metadata': (),\n",
            " 'disable_streaming': False,\n",
            " 'google_api_key': SecretStr('**********'),\n",
            " 'max_output_tokens': None,\n",
            " 'max_retries': 2,\n",
            " 'metadata': None,\n",
            " 'model': 'models/gemini-1.5-flash',\n",
            " 'n': 1,\n",
            " 'name': None,\n",
            " 'rate_limiter': None,\n",
            " 'safety_settings': None,\n",
            " 'tags': None,\n",
            " 'temperature': 0.7,\n",
            " 'timeout': None,\n",
            " 'top_k': None,\n",
            " 'top_p': None,\n",
            " 'transport': None,\n",
            " 'verbose': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(llm))  # Lists all available attributes and methods\n"
      ],
      "metadata": {
        "id": "7xez1r3Smfat",
        "outputId": "39e17756-c7e2-4fd8-9b97-e6f324c1ca2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__call__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_agenerate', '_agenerate_with_cache', '_all_required_field_names', '_astream', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_async', '_call_with_config', '_check_frozen', '_combine_llm_outputs', '_convert_input', '_copy_and_set_values', '_generate', '_generate_with_cache', '_get_invocation_params', '_get_llm_string', '_get_ls_params', '_get_value', '_identifying_params', '_is_protocol', '_iter', '_llm_type', '_model_family', '_prepare_params', '_prepare_request', '_serialized', '_should_stream', '_stream', '_supports_tool_choice', '_transform_stream_with_config', 'abatch', 'abatch_as_completed', 'additional_headers', 'agenerate', 'agenerate_prompt', 'ainvoke', 'apredict', 'apredict_messages', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'async_client', 'async_client_running', 'atransform', 'batch', 'batch_as_completed', 'bind', 'bind_tools', 'cache', 'cached_content', 'call_as_llm', 'callback_manager', 'callbacks', 'client', 'client_options', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'convert_system_message_to_human', 'copy', 'create_cached_content', 'credentials', 'custom_get_token_ids', 'default_metadata', 'dict', 'disable_streaming', 'from_orm', 'generate', 'generate_prompt', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_num_tokens', 'get_num_tokens_from_messages', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'get_token_ids', 'google_api_key', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'max_output_tokens', 'max_retries', 'metadata', 'model', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'n', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'predict', 'predict_messages', 'raise_deprecation', 'rate_limiter', 'safety_settings', 'schema', 'schema_json', 'set_verbose', 'stream', 'tags', 'temperature', 'timeout', 'to_json', 'to_json_not_implemented', 'top_k', 'top_p', 'transform', 'transport', 'update_forward_refs', 'validate', 'validate_environment', 'verbose', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_structured_output', 'with_types']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nodes**"
      ],
      "metadata": {
        "id": "Cm7_MKHYmPlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules for typing\n",
        "from typing import TypedDict\n",
        "\n",
        "# Define the TypedDictState class\n",
        "class TypedDictState(TypedDict):\n",
        "    name: str\n",
        "    mood: str\n",
        "\n",
        "def node_1(state: TypedDictState):\n",
        "    print(\"---Node 1---\")\n",
        "    return {\"name\": state['name'] + \" is ... \"}\n",
        "\n",
        "def node_2(state: TypedDictState):\n",
        "    print(\"---Node 2---\")\n",
        "    return {\"mood\": \"happy\"}\n",
        "\n",
        "def node_3(state: TypedDictState):\n",
        "    print(\"---Node 3---\")\n",
        "    return {\"mood\": \"sad\"}\n",
        "\n",
        "# Example usage\n",
        "state = TypedDictState(name=\"John\", mood=\"neutral\")\n",
        "state = node_1(state)\n",
        "print(state)\n",
        "state = node_2(state)\n",
        "print(state)\n",
        "state = node_3(state)\n",
        "print(state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB_JEh9wlV9X",
        "outputId": "29378944-b8b1-4f2b-f706-4cda672eba92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Node 1---\n",
            "{'name': 'John is ... '}\n",
            "---Node 2---\n",
            "{'mood': 'happy'}\n",
            "---Node 3---\n",
            "{'mood': 'sad'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detail of each module"
      ],
      "metadata": {
        "id": "wvfJuED92OGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Moduel 0**\n",
        "\n",
        "[Class-01: Mastering LangGraph In a New Way: Core Concepts & Implementation Node, Edges, States - Nov 8, 2024](https://www.youtube.com/watch?v=jIX9P12IkQM)"
      ],
      "metadata": {
        "id": "xhMz2sO53QMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Description of LLM methods**\n",
        "* `stream`: Stream back chunks of the response (useful for streaming results).\n",
        "* `invoke`: Call the chain on an input (provides a complete result from start to end).\n",
        "\n",
        "#### **Search Tools - TAVILY**\n",
        "\n",
        "* TAVILY is a search engine created by the LangChain open community. It provides the latest accurate data from the web.\n",
        "* LLMs do not have access to real-time information, e.g., weather forecasts.\n",
        "* LLMs do not have the latest information. That's why we use TAVILY to access real-time information, e.g., weather forecasts.\n"
      ],
      "metadata": {
        "id": "YNxG7b3K341m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the TAVILY API key from user data and set the environment variable\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "Tk6WkB-W3NAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 1 :** 2.2_chains_reducers.ipynb\n",
        "\n",
        "[Class-02: Mastering LangGraph In a New Way: LLM with Tool Calling, Chains & Reducers - Nov 9, 2024 ](https://www.youtube.com/watch?v=g1GqJx2k1Hk)\n"
      ],
      "metadata": {
        "id": "oaGxZYyY6CNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using messages as state\n",
        "\n",
        "With these foundations in place, we can now use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages) in our graph state.\n",
        "\n",
        "Let's define our state, `MessagesState`, as a `TypedDict` with a single key: `messages`.\n",
        "\n",
        "`messages` is simply a list of messages, as we defined above (e.g., `HumanMessage`, etc)."
      ],
      "metadata": {
        "id": "u0hAEpTu43tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: list[AnyMessage]"
      ],
      "metadata": {
        "id": "5R86JDLj7Rjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducers address the issue of **new state values overriding prior ones**. By using the pre-built **`add_messages` reducer**, we ensure that **messages are appended to the existing list** rather than replaced. We just need to **annotate the messages key with this reducer function as metadata**.\n",
        "\n",
        "```bash\n",
        " messages: Annotated[list[AnyMessage], add_messages]\n",
        " ```"
      ],
      "metadata": {
        "id": "rOLmVTLY7Xp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules for typing and LangGraph\n",
        "from typing import Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# Define a TypedDict class 'MessagesState' to store messages\n",
        "class MessagesState(TypedDict):\n",
        "    # Annotate the 'messages' key to use the 'add_messages' reducer function for appending messages\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n"
      ],
      "metadata": {
        "id": "yDr5tKqQ9LRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 1 :**  5_agent_memory.ipynb\n",
        "\n",
        "[Class-04: Mastering LangGraph In a New Way: Memory, RAG, Fine Tuning, GraphRAG, AI Agents - Nov 15, 2024  ](https://www.youtube.com/watch?v=Zf2C8A6RmJE)"
      ],
      "metadata": {
        "id": "rg-BpnmZzuxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from langgraph and langchain_core\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# # System message (use to increase performance)\n",
        "# Define a system message to establish the assistant's role\n",
        "# This message sets the context for the assistant's behavior\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# # Node (create assistant node)\n",
        "# Define a function named 'assistant' that takes a MessagesState as input\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "    # Invoke the LLM with tools using the system message and the current state of messages\n",
        "    # The response is returned in a dictionary format, appended to the 'messages' key\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"]) ] }\n",
        "\n",
        "    # Main :- llm_with_tools.invoke([sys_msg] + state[\"messages\"])\n",
        "    # ensures the assistant uses both the predefined context and the conversation history to respond appropriately\n",
        "\n",
        "    # step 1 : System Message Inclusion: :- llm_with_tools.invoke([sys_msg]\n",
        "    # calls the language model with just the system message, which establishes the context for the assistant.\n",
        "\n",
        "    # step 2 : Combined Messages Processing :-  llm_with_tools.invoke(state[\"messages\"])\n",
        "    # processes the current conversation history (state messages)\n",
        "    # by including them along with the system message to generate a coherent\n",
        "    # response considering both the context and the history.\n"
      ],
      "metadata": {
        "id": "JpBr_ize0XPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "```bash\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# # System message (create system message for better performance)\n",
        "sys_msg = SystemMessage(content=\"You message.\")\n",
        "```\n",
        "\n",
        "Node (create assistant node)\n",
        "\n",
        "**Functionality :** Takes input, combines it with the system message, sends them to the LLM, and stores the LLM's response.\n",
        "\n",
        "When the function gets the prompt, it sends it to the LLM along with both the system message and the prompt\n",
        "\n",
        "\n",
        "```bash\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
        "```\n",
        "\n",
        "**Detail**\n",
        "\n",
        "Define a function named 'assistant' that takes a MessagesState as input\n",
        "\n",
        "**`def assistant(state: MessagesState) -> MessagesState:`**\n",
        "\n",
        "This code calls the language model with both the system message and the current conversation history to generate a relevant response.\n",
        "\n",
        "**`[llm_with_tools.invoke([sys_msg] + state[\"messages\"]) ]`**\n",
        "\n",
        "\n",
        "**Combined Messages Processing :**\n",
        "**`[llm_with_tools.invoke([sys_msg] + state[\"messages\"])]`**\n",
        "\n",
        "This step makes sure the assistant takes into account both the initial instructions and the entire conversation so far to respond appropriately.\n",
        "\n",
        "* **Including the System Message : `(sys_msg)`** Adding the initial instruction **`(sys_msg)`** to the list of messages. This sets the context for the assistant.\n",
        "\n",
        "* **Processing Conversation History : `state[\"messages\"]`** Combining the system message with the existing conversation history (all previous messages in **`state[\"messages\"]`**). This ensures the assistant understands the context and history to generate a relevant and coherent response.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "kuCZ8ot_zx1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm note**\n",
        "\n",
        "**Don't retain memory from the initial chat!** because it don't has memory only hase state\n",
        "\n",
        "When the graph starts and runs, after finishing, the graph state is lost because it gets overwritten.\n",
        "\n",
        "**Transient State :** Starts a new chat session.\n",
        "\n",
        "**Steady State :** Remembers details from earlier conversations\n"
      ],
      "metadata": {
        "id": "OkoJwxtCGD8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "\n",
        "To retain conversations, LangGraph uses **`persistent checkpointing`**.\n",
        "\n",
        "```bash\n",
        "react_graph_memory: CompiledStateGraph = builder.compile(checkpointer=memory)\n",
        "```\n",
        "\n",
        "LangGraph automatically saves the state after each step. When the graph is invoked again using the same **`thread_id`**, it loads its saved state, allowing the chatbot to continue from where it left off\n",
        "\n",
        "**Note : Sequential tasks in LangGraph :**\n",
        "\n",
        "**LangGraph only performs parallel tasks**. To make sequential tasks, we have to design the architecture to make it sequential\n",
        "\n",
        "**State**\n",
        "\n",
        "**Note : Checkpoints are Saved in a thread:**\n",
        "\n",
        "Threads act as memory locations where the state is saved.\n",
        "\n",
        "Through the **`thread_id`**, the state is saved into the memory location.\n",
        "\n",
        "**Note : MemorySaver :**\n",
        "\n",
        "MemorySaver retains the state while connected. Once disconnected, the data is lost.\n",
        "\n",
        "\n",
        "This means the memory is not persistent and does not save the data when the connection is terminated. For persistent memory, you would need to implement a different solution. (module 2 )"
      ],
      "metadata": {
        "id": "ONUfjR5LDrkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Vdw_qSWygRo"
      },
      "outputs": [],
      "source": [
        "# Steady State : Remembers details from earlier conversations\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory: MemorySaver = MemorySaver()\n",
        "react_graph_memory: CompiledStateGraph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 1 :**  5_agent_memory.ipynb\n",
        "\n",
        "**Tutorial**\n",
        "\n",
        "When we use memory, we need to specify a `thread_id`.\n",
        "\n",
        "This **`thread_id`** will store our memory location of graph states.\n",
        "\n",
        "Here is a cartoon:\n",
        "\n",
        "* The checkpointer write the state at every step of the graph\n",
        "* These checkpoints are saved in a thread\n",
        "* We can access that thread in the future using the `thread_id`\n",
        "\n",
        "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)\n"
      ],
      "metadata": {
        "id": "IyHXPX71Nxd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}"
      ],
      "metadata": {
        "id": "3mKJfxCJZURW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify an input\n",
        "# Create a list of messages with a HumanMessage containing the content \"Add 3 and 4.\"\n",
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "\n",
        "# Run\n",
        "# Invoke the graph with the specified messages and configuration\n",
        "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
        "\n",
        "# Iterate through the resulting messages\n",
        "for m in messages['messages']:\n",
        "    # Print each message in a formatted ma\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "gqxoO52KZ3_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 2 :** 1_state_schema.ipynb"
      ],
      "metadata": {
        "id": "p4aMX_q2g74j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pydantic can perform validation to check whether data conforms to the specified types and constraints at runtime.**"
      ],
      "metadata": {
        "id": "UrmMn9uTg8Vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 2 -  2-state_reducers - Lesson 2**\n",
        "\n",
        "[Class-05: Mastering LangGraph In a New Way: Agent Deployment, State Schema, State Reducers - Nov 16, 2024 ](https://www.youtube.com/watch?v=i_WfLkZsGfs)"
      ],
      "metadata": {
        "id": "Sshi1u0Hdrky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes : Module 2 - 2-state_reducers**\n",
        "\n",
        "Isolate messages to delete\n",
        "\n",
        "**`delete_messages = [RemoveMessage(id=m.id) for m in messages[:-2]]`**\n",
        "\n",
        "\n",
        "```bash\n",
        "def filter_messages(state: MessagesState):\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"messages\": delete_messages}\n",
        "```\n",
        "\n",
        "Through slicing get last 2 message\n",
        "**`state[\"messages\"][:-2]`**\n",
        "\n",
        "save last 2 messages in messages\n",
        "**`return {\"messages\": delete_messages}`**\n"
      ],
      "metadata": {
        "id": "ZQx5LZxcU3q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolate messages to delete\n",
        "def filter_messages(state: MessagesState):\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"messages\": delete_messages}\n",
        "\n",
        "print(delete_messages)"
      ],
      "metadata": {
        "id": "995tOoozW3De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uLLHcUTbmV6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 2 - Multiple Schemas - Lesson 3**\n",
        "\n",
        "[Class-06: LangGraph - Multiple Schemas, Filter & Trim Messages, and Introduction to RAG - Nov 21, 2024](https://www.youtube.com/watch?v=rBPZwQ7jlBo&list=PL0vKVrkG4hWoHDg46N85-9NDhmOaPWEwA&index=6)"
      ],
      "metadata": {
        "id": "deuHCZtrdgxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm note **\n",
        "\n",
        "**Module 2 - Multiple Schemas - Lesson 3**\n",
        "\n",
        "1. We can change the schema (we can define input/output).\n",
        "\n",
        "2. We can use the private schema.\n",
        "\n",
        "**Note :** Private isn't defined when Build the graph,only  define OverallState, input, output\n",
        "\n",
        "Private isn't defined when building the graph; only defined\n",
        "* OverallState\n",
        "* input\n",
        "* output\n",
        "\n",
        "are defined.\n",
        "\n",
        "**`\n",
        "graph = StateGraph(OverallState, input=InputState, output=OutputState)`**\n",
        "\n",
        "**`OverallState` is default schema** it doesn't require a positional argument"
      ],
      "metadata": {
        "id": "S1xk0VQJdtpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 2 - trim-filter-messages - Lesson 4**\n",
        "\n",
        "[Class-06: LangGraph - Multiple Schemas, Filter & Trim Messages, and Introduction to RAG - Nov 21, 2024](https://www.youtube.com/watch?v=rBPZwQ7jlBo&list=PL0vKVrkG4hWoHDg46N85-9NDhmOaPWEwA&index=7)"
      ],
      "metadata": {
        "id": "jGyR92oQdGup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "### Module 2 : 4_trim-filter-messages\n",
        "\n",
        "[ Streaming ](https://langchain-ai.github.io/langgraph/concepts/streaming/#streaming-graph-outputs-stream-and-astream)\n",
        "\n",
        "**Streaming LLM tokens and events (`.astream_events`)**\n",
        "\n",
        "The **`.astream_events()`** method streams events in real-time as they occur within nodes, making it useful for tracking LLM token updates. It helps monitor and view these events during the graph's execution."
      ],
      "metadata": {
        "id": "LWYRVTK9dHjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async for m in graph.astream_events({'messages': messages}, version=\"v2\"): # The version argument is now correctly passed as a keyword argument.\n",
        "      print(m)\n",
        "      print(\"\\n--------------\\n\")\n",
        "\n",
        "      # on_chain_start : start of a node's ececusion\n",
        "      # on_chain_end : incdicates the completion of nodes's execusion\n",
        "      # on_chain_stream : represent intermediate data or progress updates during a node's ececustion"
      ],
      "metadata": {
        "id": "Xq03gMqzgPRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 2 :** 6.0_chatbot_external_memory,\n",
        " 6.1_chatbot_external_memory_with_our_db_file.ipynb, 6.2_chatbot_external_memory_with_postgres.ipynb\n",
        "\n",
        "[Class08:LangGraph: External Memory with External DB, Introduction to Agentic RAG - Nov 23, 2024](https://www.youtube.com/watch?v=ooU238RcOdo)"
      ],
      "metadata": {
        "id": "pm7uP7GM6UWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**.\n",
        "\n",
        "### Module 2 : 6.1_chatbot_external_memory_with_our_db_file.ipynb\n",
        "\n",
        "\n",
        "In the old functionality, the script only shows the default messages and does not append new messages after disconnecting from the environment. To save our new data into a script file and persist it, we need to update the script. We can then save the updated file to the drive or download it to our system, and use the new appended file as required.\n",
        "\n",
        "To append our newly updated data into a file and either save it to the drive or our system, we have three use cases:\n",
        "\n",
        "Save a newly updated file with its name and designation either to the system or drive, and then proceed with further functionality from it.\n",
        "\n",
        "**use case**\n",
        "1. Fetching a Database File from a GitHub URL\n",
        "2. Uploading a Database File from Your Local System\n",
        "3. Accessing a Database File from Google Drive"
      ],
      "metadata": {
        "id": "kG842Xo96KsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "### Module 2 :  6.2_chatbot_external_memory_with_postgres.ipynb\n",
        "\n",
        "To have a **chatbot with persistent memory**, use **checkpointers that support external databases**\n",
        "\n",
        "**`chatbot_memory_db` :**\n",
        "We use the Neon database to save data.\n",
        "\n",
        "\n",
        "**rm notes** **Module 2 :**  6.2_chatbot_external_memory_with_postgres.ipynb\n",
        "\n",
        "To have a **chatbot with persistent memory**, use **checkpointers that support external databases**\n",
        "\n",
        "**`chatbot_memory_db` :**\n",
        "We use the Neon database to save data.\n",
        "\n",
        "**Retrive Chat history**\n",
        "\n",
        "```\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state\n",
        "```\n",
        "\n",
        "**get messages:- graph.get_state(config).values.get(\"messages\")**\n",
        "\n",
        "step1 :- get key of messages from snapshot\n",
        "\n",
        "```\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "messages = graph.get_state(config).values.get(\"messages\")\n",
        "graph_state\n",
        "```\n",
        "Require steaming :-\n",
        "\n",
        "**Module 3 :** 1_streaming-interruption.ipynb\n"
      ],
      "metadata": {
        "id": "--_x9ZND6Gct"
      }
    }
  ]
}
