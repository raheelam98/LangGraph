{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTOZrl9AuWgJ2eAUEzZSHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/LangGraph/blob/main/langgraph_code_documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm note**\n",
        "\n",
        "state:- structure of work flow\n",
        "\n",
        "memory :- presisten memory (store things)\n",
        "\n",
        "API :- talk one meachine to another meachine"
      ],
      "metadata": {
        "id": "Qr-iH133ueA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ## langchain-academy\n",
        "\n",
        "[LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph)\n",
        "  \n",
        "[langchain-academy - Github](https://github.com/langchain-ai/langchain-academy/tree/main)"
      ],
      "metadata": {
        "id": "b_KU43rO4lh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required packages:\n",
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langsmith # check\n",
        "%pip install --quiet -U langchain_google_genai langchain_core langgraph"
      ],
      "metadata": {
        "id": "VTp5u3K70cwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description of Libraries**\n",
        "* `langchain-google-genai:` LangChain doesn't have its own chat model, so we use Google's Gemini.\n",
        "* `langchain-core`: Utilizes LangChain core libraries, e.g., human-message, ai-message.\n",
        "* `langchain_community:` Tavily-python is part of the LangChain community package.\n",
        "* `tavily-python: `Used for web search.\n",
        "Note: Use both langchain_community and tavily-python together.\n"
      ],
      "metadata": {
        "id": "6kcY9QzS5adc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"project_name\""
      ],
      "metadata": {
        "id": "kJqgQQ3d0lN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys\n",
        "# Get the GEMINI API key from user data\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "nWMujpLX1TBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ],
      "metadata": {
        "id": "krx7ywtn1eGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "LANGCHAIN_API_KEY= userdata.get('LANGCHAIN_API_KEY')"
      ],
      "metadata": {
        "id": "dROJZ5v-1UOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ChatGoogleGenerativeAI with the Gemini model\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    max_retries=2,\n",
        "    api_key=gemini_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "bsAhvpzJ1n-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LLM with a query\n",
        "result = llm.invoke(\"Rain is expected in Karachi starting today?\")\n",
        "result"
      ],
      "metadata": {
        "id": "h071NcYm1pYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code is to create a chatbot using the LangChain framework, integrating tools for search results, memory checkpointing (part 3), interrup_node (part4)**"
      ],
      "metadata": {
        "id": "f7ekjFLW1_Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Annotated\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver  # Import memory checkpointing\n",
        "from langgraph.graph import StateGraph  # Import StateGraph to build the graph\n",
        "from langgraph.graph.message import add_messages  # Import add_messages function\n",
        "from langgraph.prebuilt import ToolNode  # Import ToolNode\n",
        "\n",
        "# Define State with messages\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Initialize the StateGraph with State\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Initialize the search tool and list of tools\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "\n",
        "# Initialize the language model and bind it with tools\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# Define the chatbot function to invoke the language model\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Add the chatbot node to the graph\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Add the tools node to the graph\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Add conditional edges between nodes\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "\n",
        "# Add an edge from the tools node to the chatbot node\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "\n",
        "# Set the chatbot as the entry point of the graph\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "# Compile the graph with memory checkpointing  - part 3\n",
        "#graph = graph_builder.compile(checkpointer=MemorySaver())\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "#interrupt_before=[\"tools\"],  # This is new!   -  part 4\n",
        "# Note: can also interrupt **after** actions, if desired.\n",
        "# interrupt_after=[\"tools\"]\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"tools\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "j0AHc_No19i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detail of each module"
      ],
      "metadata": {
        "id": "wvfJuED92OGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Moduel 0**\n",
        "\n",
        "[Class-01: Mastering LangGraph In a New Way: Core Concepts & Implementation Node, Edges, States - Nov 8, 2024](https://www.youtube.com/watch?v=jIX9P12IkQM)"
      ],
      "metadata": {
        "id": "xhMz2sO53QMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Description of LLM methods**\n",
        "* `stream`: Stream back chunks of the response (useful for streaming results).\n",
        "* `invoke`: Call the chain on an input (provides a complete result from start to end).\n",
        "\n",
        "#### **Search Tools - TAVILY**\n",
        "\n",
        "* TAVILY is a search engine created by the LangChain open community. It provides the latest accurate data from the web.\n",
        "* LLMs do not have access to real-time information, e.g., weather forecasts.\n",
        "* LLMs do not have the latest information. That's why we use TAVILY to access real-time information, e.g., weather forecasts.\n"
      ],
      "metadata": {
        "id": "YNxG7b3K341m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the TAVILY API key from user data and set the environment variable\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "Tk6WkB-W3NAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Module 1 :**  5_agent_memory.ipynb\n",
        "\n",
        "[Class-04: Mastering LangGraph In a New Way: Memory, RAG, Fine Tuning, GraphRAG, AI Agents - Nov 15, 2024  ](https://www.youtube.com/watch?v=Zf2C8A6RmJE)"
      ],
      "metadata": {
        "id": "rg-BpnmZzuxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from langgraph and langchain_core\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# # System message (use to increase performance)\n",
        "# Define a system message to establish the assistant's role\n",
        "# This message sets the context for the assistant's behavior\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# # Node (create assistant node)\n",
        "# Define a function named 'assistant' that takes a MessagesState as input\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "    # Invoke the LLM with tools using the system message and the current state of messages\n",
        "    # The response is returned in a dictionary format, appended to the 'messages' key\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"]) ] }\n",
        "\n",
        "    # Main :- llm_with_tools.invoke([sys_msg] + state[\"messages\"])\n",
        "    # ensures the assistant uses both the predefined context and the conversation history to respond appropriately\n",
        "\n",
        "    # step 1 : System Message Inclusion: :- llm_with_tools.invoke([sys_msg]\n",
        "    # calls the language model with just the system message, which establishes the context for the assistant.\n",
        "\n",
        "    # step 2 : Combined Messages Processing :-  llm_with_tools.invoke(state[\"messages\"])\n",
        "    # processes the current conversation history (state messages)\n",
        "    # by including them along with the system message to generate a coherent\n",
        "    # response considering both the context and the history.\n"
      ],
      "metadata": {
        "id": "JpBr_ize0XPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "```bash\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# # System message (create system message for better performance)\n",
        "sys_msg = SystemMessage(content=\"You message.\")\n",
        "```\n",
        "\n",
        "Node (create assistant node)\n",
        "\n",
        "**Functionality :** Takes input, combines it with the system message, sends them to the LLM, and stores the LLM's response.\n",
        "\n",
        "When the function gets the prompt, it sends it to the LLM along with both the system message and the prompt\n",
        "\n",
        "\n",
        "```bash\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
        "```\n",
        "\n",
        "**Detail**\n",
        "\n",
        "Define a function named 'assistant' that takes a MessagesState as input\n",
        "\n",
        "**`def assistant(state: MessagesState) -> MessagesState:`**\n",
        "\n",
        "This code calls the language model with both the system message and the current conversation history to generate a relevant response.\n",
        "\n",
        "**`[llm_with_tools.invoke([sys_msg] + state[\"messages\"]) ]`**\n",
        "\n",
        "\n",
        "**Combined Messages Processing :**\n",
        "**`[llm_with_tools.invoke([sys_msg] + state[\"messages\"])]`**\n",
        "\n",
        "This step makes sure the assistant takes into account both the initial instructions and the entire conversation so far to respond appropriately.\n",
        "\n",
        "* **Including the System Message : `(sys_msg)`** Adding the initial instruction **`(sys_msg)`** to the list of messages. This sets the context for the assistant.\n",
        "\n",
        "* **Processing Conversation History : `state[\"messages\"]`** Combining the system message with the existing conversation history (all previous messages in **`state[\"messages\"]`**). This ensures the assistant understands the context and history to generate a relevant and coherent response.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "kuCZ8ot_zx1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm note**\n",
        "\n",
        "**Don't retain memory from the initial chat!** because it don't has memory only hase state\n",
        "\n",
        "When the graph starts and runs, after finishing, the graph state is lost because it gets overwritten.\n",
        "\n",
        "**Transient State :** Starts a new chat session.\n",
        "\n",
        "**Steady State :** Remembers details from earlier conversations\n"
      ],
      "metadata": {
        "id": "OkoJwxtCGD8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rm notes**\n",
        "\n",
        "\n",
        "To retain conversations, LangGraph uses **`persistent checkpointing`**.\n",
        "\n",
        "```bash\n",
        "react_graph_memory: CompiledStateGraph = builder.compile(checkpointer=memory)\n",
        "```\n",
        "\n",
        "LangGraph automatically saves the state after each step. When the graph is invoked again using the same **`thread_id`**, it loads its saved state, allowing the chatbot to continue from where it left off\n",
        "\n",
        "**Note : Sequential tasks in LangGraph :**\n",
        "\n",
        "**LangGraph only performs parallel tasks**. To make sequential tasks, we have to design the architecture to make it sequential\n",
        "\n",
        "**State**\n",
        "\n",
        "**Note : Checkpoints are Saved in a thread:**\n",
        "\n",
        "Threads act as memory locations where the state is saved.\n",
        "\n",
        "Through the **`thread_id`**, the state is saved into the memory location.\n",
        "\n",
        "**Note : MemorySaver :**\n",
        "\n",
        "MemorySaver retains the state while connected. Once disconnected, the data is lost.\n",
        "\n",
        "\n",
        "This means the memory is not persistent and does not save the data when the connection is terminated. For persistent memory, you would need to implement a different solution. (module 2 )"
      ],
      "metadata": {
        "id": "ONUfjR5LDrkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8Vdw_qSWygRo"
      },
      "outputs": [],
      "source": [
        "# Steady State : Remembers details from earlier conversations\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory: MemorySaver = MemorySaver()\n",
        "react_graph_memory: CompiledStateGraph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module 1 :**  5_agent_memory.ipynb\n",
        "\n",
        "**Tutorial**\n",
        "\n",
        "When we use memory, we need to specify a `thread_id`.\n",
        "\n",
        "This **`thread_id`** will store our memory location of graph states.\n",
        "\n",
        "Here is a cartoon:\n",
        "\n",
        "* The checkpointer write the state at every step of the graph\n",
        "* These checkpoints are saved in a thread\n",
        "* We can access that thread in the future using the `thread_id`\n",
        "\n",
        "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)\n"
      ],
      "metadata": {
        "id": "IyHXPX71Nxd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}"
      ],
      "metadata": {
        "id": "3mKJfxCJZURW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify an input\n",
        "# Create a list of messages with a HumanMessage containing the content \"Add 3 and 4.\"\n",
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "\n",
        "# Run\n",
        "# Invoke the graph with the specified messages and configuration\n",
        "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
        "\n",
        "# Iterate through the resulting messages\n",
        "for m in messages['messages']:\n",
        "    # Print each message in a formatted ma\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "gqxoO52KZ3_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}